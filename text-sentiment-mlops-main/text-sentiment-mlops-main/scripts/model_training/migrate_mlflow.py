import pandas as pd
import mlflow
import mlflow.pyfunc
from pathlib import Path
import json
import random
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
import numpy as np

# Define paths
EXPERIMENTS_DIR = Path("/Users/saliksaleem/Desktop/Projects/text-sentiment-mlops/models/experiments")
LOG_FILE = EXPERIMENTS_DIR / "experiment_log.csv"
TEST_DATA_DIR = Path("/Users/saliksaleem/Desktop/Projects/text-sentiment-mlops/data/processed")

# Define minimum performance thresholds for production models
PRODUCTION_THRESHOLDS = {
    "accuracy": 0.10,  # Lowered thresholds for demo purposes
    "f1": 0.15,
    "precision": 0.15,
    "recall": 0.15
}
import dagshub
dagshub.init(repo_owner='saleemsalik786', repo_name='my-first-repo', mlflow=True)
# Custom PyFunc model for MLflow
class DummySentimentModel(mlflow.pyfunc.PythonModel):
    def __init__(self, experiment_id=None, metrics=None):
        self.experiment_id = experiment_id
        self.metrics = metrics or {}
    
    def predict(self, context, model_input):
        """
        Simple dummy prediction function.
        For text input, returns 1 if text length is even, 0 otherwise.
        For DataFrame input, creates predictions based on index.
        """
        if isinstance(model_input, pd.DataFrame):
            # Just a simple deterministic function for demo
            return model_input.index % 2
        elif isinstance(model_input, list):
            return [1 if len(str(x)) % 2 == 0 else 0 for x in model_input]
        else:
            return 1 if len(str(model_input)) % 2 == 0 else 0

def create_simple_test_data():
    """Create simple test data for demonstration purposes"""
    # This creates a small synthetic test set with 100 examples
    # You can replace this with actual data loading if you have test data
    
    # Create a directory for the test data if it doesn't exist
    TEST_DATA_DIR.mkdir(parents=True, exist_ok=True)
    
    # Create simple test data with text and sentiment labels
    test_data = {
        "id": list(range(1, 101)),
        "text": [f"Test sample {i}" for i in range(1, 101)],
        "label": [random.randint(0, 1) for _ in range(100)]  # Binary sentiment labels
    }
    
    test_df = pd.DataFrame(test_data)
    test_file = TEST_DATA_DIR / "simple_test.csv"
    test_df.to_csv(test_file, index=False)
    print(f"Created simple test data at {test_file}")
    
    return test_df

def simulate_model_evaluation(model_id):
    """
    Simulate model evaluation without relying on actual model inference.
    This is a placeholder that returns reasonable metrics based on model_id.
    """
    # For demonstration purposes, we'll return slightly different metrics for each model
    # In a real scenario, you would load and evaluate the actual model
    
    # Use model_id as a seed for reproducible "random" metrics
    seed = hash(model_id) % 10000
    random.seed(seed)
    
    # Generate simulated metrics - better models will have higher experiment numbers
    # This is just for demonstration
    base_accuracy = 0.70 + (int(model_id[-2:]) % 100) / 100 * 0.25
    
    metrics = {
        "accuracy": min(0.98, base_accuracy + random.uniform(-0.05, 0.05)),
        "f1": min(0.98, base_accuracy + random.uniform(-0.07, 0.03)),
        "precision": min(0.98, base_accuracy + random.uniform(-0.03, 0.07)),
        "recall": min(0.98, base_accuracy + random.uniform(-0.06, 0.04))
    }
    
    # Round to 4 decimal places for cleaner output
    return {k: round(v, 4) for k, v in metrics.items()}

def passes_production_criteria(metrics):
    """Check if model metrics pass production thresholds"""
    for metric_name, threshold in PRODUCTION_THRESHOLDS.items():
        if metrics.get(metric_name, 0) < threshold:
            return False
    return True

def migrate_and_evaluate_experiments():
    """Migrate experiments to MLflow and evaluate for production"""
    # Read experiment log
    df = pd.read_csv(LOG_FILE)
    print(f"Found {len(df)} experiments in log file")
    print(f"Available columns: {df.columns.tolist()}")
    
    # Connect to MLflow
    mlflow.set_tracking_uri("http://127.0.0.1:5000/")
    mlflow.set_experiment("Sentiment_Analysis")
    
    # Create test data
    test_data = create_simple_test_data()
    print(f"Created test data with {len(test_data)} samples")
    
    production_candidates = []
    
    # Process each experiment
    for idx, row in df.iterrows():
        # Get experiment ID - handle both 'experiment_id' column name variations
        if 'experiment_id' in row:
            experiment_id = str(row['experiment_id'])
        else:
            # Try to find alternative column that might contain experiment ID
            possible_id_columns = [col for col in row.index if 'id' in col.lower()]
            if possible_id_columns:
                experiment_id = str(row[possible_id_columns[0]])
            else:
                # Generate an ID if none found
                experiment_id = f"exp_{idx}"
        
        print(f"\nProcessing experiment {experiment_id} ({idx+1}/{len(df)})")
        
        try:
            # Start MLflow run
            with mlflow.start_run(run_name=f"exp_{experiment_id}"):
                run_id = mlflow.active_run().info.run_id
                print(f"MLflow run ID: {run_id}")
                
                # Log available parameters
                for key in row.keys():
                    # Skip metrics and other non-parameter columns
                    if key in ['experiment_id', 'training_time_seconds',
                             'train_samples', 'test_samples', 'eval_accuracy', 
                             'eval_f1', 'eval_precision', 'eval_recall']:
                        continue
                    
                    # Handle various data types properly
                    try:
                        value = row[key]
                        if pd.isna(value):
                            continue
                        mlflow.log_param(key, value)
                    except Exception as e:
                        print(f"Error logging parameter {key}: {e}")
                
                # Log metrics from experiment log
                for metric in ['training_time_seconds', 'train_samples', 'test_samples',
                             'eval_accuracy', 'eval_f1', 'eval_precision', 'eval_recall']:
                    if metric in row and not pd.isna(row[metric]):
                        mlflow.log_metric(metric, row[metric])
                
                # Simulate model evaluation instead of loading the model
                # This avoids the need for actual model files for testing the workflow
                print("Simulating test evaluation...")
                test_metrics = simulate_model_evaluation(experiment_id)
                
                # Log test metrics
                for metric_name, value in test_metrics.items():
                    mlflow.log_metric(f"test_{metric_name}", value)
                
                # Create a simple dummy model
                class DummySentimentModel:
                    def __init__(self, experiment_id):
                        self.experiment_id = experiment_id
                        self.metrics = test_metrics
                    
                    def predict(self, text):
                        # Simple deterministic prediction based on text length
                        return 1 if len(text) % 2 == 0 else 0
                
                # Create and save the dummy model
                dummy_model = DummySentimentModel(experiment_id)
                
                # Log the model to MLflow
                print("Logging model to MLflow...")
                mlflow.pyfunc.log_model(
                    artifact_path="model",
                    python_model=dummy_model,
                    registered_model_name=f"SentimentModel_{experiment_id}"
                )
                
                # Check if model meets production criteria
                if passes_production_criteria(test_metrics):
                    print(f"âœ… Model passes production criteria")
                    # Model is already registered above, now track it for promotion
                    model_name = f"SentimentModel_{experiment_id}"
                    
                    # Get the version from the registry
                    client = mlflow.tracking.MlflowClient()
                    versions = client.get_latest_versions(model_name)
                    if versions:
                        version = versions[0].version
                        print(f"Registered model {model_name} version {version}")
                        
                        production_candidates.append({
                            'experiment_id': experiment_id,
                            'model_name': model_name,
                            'version': version,
                            'metrics': test_metrics
                        })
                    else:
                        print(f"Warning: Could not find version for {model_name}")
                else:
                    print(f"âŒ Model does not meet production criteria")
                    print(f"Test metrics: {test_metrics}")
                    print(f"Required thresholds: {PRODUCTION_THRESHOLDS}")
        except Exception as e:
            print(f"Error processing experiment {experiment_id}: {e}")
            import traceback
            traceback.print_exc()
    
    # Promote best model to production
    if production_candidates:
        print("\n=== Production Candidates ===")
        for i, candidate in enumerate(production_candidates):
            print(f"{i+1}. Experiment: {candidate['experiment_id']}")
            print(f"   Model: {candidate['model_name']} (v{candidate['version']})")
            print(f"   Metrics: {candidate['metrics']}")
        
        # Find best model based on F1 score (modify as needed)
        best_candidate = max(production_candidates, 
                           key=lambda x: x['metrics'].get('f1', 0))
        
        # Actually transition the model to production
        try:
            client = mlflow.tracking.MlflowClient()
            client.transition_model_version_stage(
                name=best_candidate['model_name'],
                version=best_candidate['version'],
                stage="Production"
            )
            
            print(f"\nðŸš€ Model {best_candidate['model_name']} (v{best_candidate['version']}) " + 
                  f"promoted to PRODUCTION")
            print(f"   Metrics: {best_candidate['metrics']}")
        except Exception as e:
            print(f"Error promoting model to production: {e}")
            import traceback
            traceback.print_exc()
    else:
        print("\nâš ï¸ No models met the production criteria")

if __name__ == "__main__":
    migrate_and_evaluate_experiments()